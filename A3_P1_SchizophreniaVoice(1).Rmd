---
title: "Assignment 2 Part1 - Voice In Schizophrenia"
author: "Riccardo Fusaroli"
date: "July 17, 2016"
output: html_document
---

## Assignment 2 - Part 1 - Assessing voice in schizophrenia

Schizophrenia has been associated with "inappropriate" voice, sometimes monotone, sometimes croaky. A few studies indicate that pitch might be an index of schizophrenia. However, an ongoing meta-analysis of the literature (which you will have a go at in the last assignment) indicates that pitch mean and standard deviation are only weak indicators of diagnosis. Can we do better with our new fancy complex skills?

The corpus you are asked to analyse is a set of voice recordings from people with schizophrenia (just after first diagnosis) and 1-1 matched controls (on gender, age, education). Each participant watched 10 videos of triangles moving across the screen and had to describe them (so you have circa 10 recordings per person). I have already extracted the pitch once every 10 milliseconds and you will have to use this data to assess differences in the voice.

N.B. Question to be answered via email to Celine: can you characterize voice in schizophrenia as acoustically different? Report the methods you used to answer this question and the results from the analyses. Add a couple of lines trying to interpret the results (make sense of the difference). E.g. People with schizophrenia tend to have high-pitched voice, and present bigger swings in their prosody than controls. Add a couple of lines describing limitations of the data/analyses if any is relevant.

N.B. There are looots of files to be dealt with. Probably too many for your computer. This is a challenge for you. Some (complementary) possible strategies: You can select a subset of files only (and you have to justify your choice). You can learn how to use the apply() or map() functions. You can coordinate with classmates.

1. In the course of this assignment you have to first select one datafile and figure out how to:

```{r}
#load all libraries here
library(plyr)
library(crqa)
library(magrittr)


#Load one datafile for excercise 1 and testing functions
data_1 = read.table("Pitch/Study2D0S209T4_f0.txt", header = T)

#save names of all files in Pitch folder (in format: Pitch/Study...txt so it can be loaded without changing wd)
pitch_files=list.files(path="Pitch",full.names = T)
```

```{r}
#get info from the name of the file
df_name = "Pitch/Study4D1S420T4_f0.txt"

#cut off string and keep only from 16th letter to 18th
Subject=as.numeric(substr(df_name, 16, 18))

Diagnosis=as.factor(substr(df_name,14,14))

Trial=substr(df_name,20,21)

Study=as.numeric(substr(df_name,12,12))

#bind together into dataframe
info=data.frame(Subject,Diagnosis,Trial,Study)
```

- Extract "standard" descriptors of pitch: Mean, standard deviation, range
```{r}
#get standard descriptors
mean=mean(data_1$f0)

sd=sd(data_1$f0)

#gets a vector of minimum and maximum
range=range(data_1$f0)
min=range[1] #first number in vector is minimun
max=range[2]

#bind together
standard=data.frame(mean,sd,min,max)
```

- Extract less "standard" descriptors of pitch you can think of (e.g. median, iqr, mean absoluted deviation, coefficient of variation)
```{r}
median=median(data_1$f0)

iqr=IQR(data_1$f0)

abs_mean_dev=mean(abs(data_1$f0-mean(data_1$f0)))

cof_var = sd(data_1$f0, na.rm=TRUE)/mean(data_1$f0, na.rm=TRUE)*100 #get coefficient of variation as percent

#bind together
less_standard=data.frame(median,iqr,abs_mean_dev,cof_var)
```


- Extract "complex" descriptors: recurrence quantification analysis
```{r}
#list needed for optimizeParam function
par = list(lgM =  50, steps = seq(1, 6, 1),  radiusspan = 100,  radiussample = 40, normalize = 0,  rescale = 0,  mindiagline = 2,  minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE,  fnnpercent = 10,  typeami = "mindip")

#get parameters for rqa - delay, emddim and radius
parameters = optimizeParam(data_1$f0,data_1$f0, par, min.rec = 3.5, max.rec = 4.5)
parameters

#perform rqa - file needs to be there twice because crqa is designed for comparing two timeseries)
results=crqa(data_1$f0,data_1$f0,delay=parameters$delay,embed=parameters$emddim,radius=parameters$radius,normalize=0,rescale=0,mindiagline = 2,minvertline = 2)
names(results)

#save rqa variables to separate variables
RR = results$RR
DET = results$DET
maxL = results$maxL #maximal trajectory
L = results$L #mean trajectory
ENTR = results$ENTR
LAM=results$LAM
TT = results$TT

#bind together
rqa = data.frame(RR, DET, maxL, L, ENTR, LAM, TT)
```

```{r}
#cbind extracted info and merge with demo data
data_desc=cbind(info, standard, less_standard, rqa)
```

2. Second you will have to turn the code into a function and loop through all the files (or even better use apply/sapply/lapply)
- Remember to extract the relevant information from the file names (Participant, Diagnosis, Trial, Study)

```{r}
#1. Wrap all the code from excercise 1 to functions
# extract info about subject from name of the file
get_info=function(file_name) {
  
  Subject=as.numeric(substr(file_name, 16, 18))
  Diagnosis=as.factor(substr(file_name,14,14))
  Trial=substr(file_name,20,21)
  Study=as.numeric(substr(file_name,12,12))
  
  info_df=data.frame(Subject,Diagnosis,Trial,Study)
  
  return(info_df)
}

#test the function 
get_info(df_name) #Works
```

```{r}
#Chunk 1: turn that code to a function
#Function 1: extract standard descriptors
get_stand_desc= function (df) {
  
  mean=mean(df$f0)
  sd=sd(df$f0)
  range=range(df$f0)
  min=range[1]
  max=range[2]
  
  stand_desc=data.frame(mean,sd,min,max)
  
  return(stand_desc)
  
}

#test function
get_stand_desc(data_1) #Works
```

```{r}
#Chunk 2: turned to function

get_less_stand_desc = function (df) {
  median=median(df$f0)
  iqr=IQR(df$f0)
  abs_mean_dev=mean(abs(df$f0-mean(df$f0)))
  cof_var = sd(df$f0, na.rm=TRUE)/mean(df$f0, na.rm=TRUE)*100
  less_stand_desc=data.frame(median,iqr,abs_mean_dev,cof_var)
  return(less_stand_desc)
}

#test function
get_less_stand_desc(data_1) #Works
```

```{r}
#rqa needs first the list of parameters - get optimal parameters here

#LOOP through all datafiles and get parameters from them - then get mean or median of all parameters to get the optimal ones that can be used for all datafiles

param_all=data.frame()

for (i in pitch_files) {
  data=read.table(file=i,header=T)
  
  #list needed for running the optimizeParam function
  par = list(lgM =  50, steps = seq(1, 6, 1),  radiusspan = 100,  radiussample = 40, normalize = 0,  rescale = 0,  mindiagline = 2, minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE,  fnnpercent = 10,  typeami = "mindip")
  
  param = 
    tryCatch(
    
      { #this is the try function - if it fails then error function will be run
        optimizeParam(data$f0,data$f0, par, min.rec = 3.5, max.rec = 4.5)
    },
   #move to this part if the try part fails
     error=function(cond){
        #return results with only NAs
        parameters_fail=list(radius=NA,emddim=NA,delay=NA)
        return(parameters_fail)
        
      }) #end of tryCatch
  
  #save output of optimizeParam to variables - either 
  radius=param$radius
  emddim=param$emddim
  delay=param$delay
  
  #bind these variables to dataframe
  param_df=data.frame(radius,emddim,delay)
  param_all=rbind(param_df,param_all)
}

#remove NAs from output of the loop
param_all = na.omit(param_all)

#summarize param_all to final df with mean
parameters_mean = param_all %>%
  summarize(radius=mean(radius),emddim=mean(emddim),delay=mean(delay))

#with median => seems better - numbers are similar but emddim is without decimals :-)
parameters_median = param_all %>%
  summarize(radius=median(radius),emddim=median(emddim),delay=median(delay))

#look at results
parameters_mean #taking delay from here - shorter delay can reduce number of NAs because some ts are very short, shorter than delay
parameters_median #taking emddim because it's without decimals - needs to be and also radius taking from here because its lower and therefore should maintain RR closer to 4%

parameters_final = list(radius=parameters_median$radius ,emddim=parameters_median$emddim ,delay=parameters_mean$delay)
```

```{r}
#Chunk 3 turned into function +tryCatch implented not to crash the loop
get_rqa= function (df,ans) {
  results=
    tryCatch(
      #this is the try part if it gets error here it will move to the error part
      {crqa(df$f0,df$f0,delay=ans$delay,embed=ans$emddim,radius=ans$radius,normalize=0,rescale=0,mindiagline = 2,minvertline = 2)
        },
      #error part - if function fails this function will be executed instead
      error=function(cond){
        #return results with only NAs
        results_fail=data.frame(RR=NA,DET=NA,maxL=NA,L=NA,ENTR=NA,LAM=NA,TT=NA)
        return(results_fail)
      }
  )
  RR = results$RR
  DET = results$DET
  maxL = results$maxL #maximal trajectory
  L = results$L #mean trajectory
  ENTR = results$ENTR
  LAM=results$LAM
  TT = results$TT
  rqa_df = data.frame(RR,DET,maxL,L,ENTR,LAM,TT)
  return(rqa_df)
}

#test the function
get_rqa(data_1,parameters_final) #Works and also RR is close to 4% with the common parameters
```

```{r}
#let's loop through all files, extract the variables and rbind them all to one mega dataframe
data_all=data.frame()

for(i in pitch_files) {
  data=read.table(file=i,header=T)
  info=get_info(i)
  standard=get_stand_desc(data)
  less_standard=get_less_stand_desc(data)
  rqa=get_rqa(data,parameters_final)
  complete_df=cbind(info,standard,less_standard,rqa)
  data_all=rbind(data_all,complete_df)
}

#save the data to csv file so I don't need to run the loop again
write.csv(data_all, file = "data_analysis.csv",row.names = F)
```

```{r}
#load the final data to be independent of the first part of the code
final_data = read.csv("data_analysis.csv",header=T)

#load demographic data
demo = read.table("DemoData.txt", header= T)

#

#the final data need some tidying before merging with demo

#turn everything that should be factor to factor and numeric what should be numeric
final_data$Diagnosis= as.factor(final_data$Diagnosis)

#rename levels of Diagnosis from 1 and 0 to Scizophrenia and Control
final_data$Diagnosis = plyr::revalue(final_data$Diagnosis, c('0'="Control", "1"="Schizophrenia"))

#remove the underscore from the trial numbers
library(stringr)
final_data$Trial=str_replace(final_data$Trial,"_","")

#merge with demo data
complete_data = merge(final_data,demo, by=c("Subject", "Diagnosis", "Study"))

#combine ID and Diagnosis to new column to distinguish between schizophrenics and controls (each pair has the same ID)
library(tidyr)
complete_data= unite(complete_data, ID, c(Subject,Diagnosis),remove=F,sep="_")

#save the complete file just to be sure it doesn't go missing
write.csv(complete_data, "complete_data.csv",row.names = F)
```

3. Make one model per acoustic feature and test whether you can observe significant difference due to Diagnosis. Tip: Which other fixed factors should you control for (that is, include in the model)? Which random ones?

```{r}
library(lmerTest)
complete_data$ID=as.factor(complete_data$ID)
```

- Bonus points: cross-validate the model and report the betas and standard errors from all rounds to get an idea of how robust the estimates are. 
3a. Is study a significant predictor in these models? What should you infer from this? Does study interact with diagnosis? What should you infer from this?

4. Bonus Question: Compare effect size of diagnosis across the different measures. Which measure seems most sensitive?
- Tip: to compare across measures you need to put all of them on the same scale, that is, you need to "standardize" them (z-score)

5. Bonus question. In the Clinical Info file you have additional information about the participants. Which additional parameters (e.g. age, gender) should we control for? Report the effects.

6. Write a paragraph reporting methods and results

[Next assignment: can we use these measures to build a tool that diagnoses people from voice only?]

## N.B. Remember to save the acoustic features of voice in a separate file, so to be able to load them next time