---
title: "Assignment 2 Part1 - Voice In Schizophrenia"
author: "Riccardo Fusaroli"
date: "July 17, 2016"
output: html_document
---

## Assignment 2 - Part 1 - Assessing voice in schizophrenia

Schizophrenia has been associated with "inappropriate" voice, sometimes monotone, sometimes croaky. A few studies indicate that pitch might be an index of schizophrenia. However, an ongoing meta-analysis of the literature (which you will have a go at in the last assignment) indicates that pitch mean and standard deviation are only weak indicators of diagnosis. Can we do better with our new fancy complex skills?

The corpus you are asked to analyse is a set of voice recordings from people with schizophrenia (just after first diagnosis) and 1-1 matched controls (on gender, age, education). Each participant watched 10 videos of triangles moving across the screen and had to describe them (so you have circa 10 recordings per person). I have already extracted the pitch once every 10 milliseconds and you will have to use this data to assess differences in the voice.

N.B. Question to be answered via email to Celine: can you characterize voice in schizophrenia as acoustically different? Report the methods you used to answer this question and the results from the analyses. Add a couple of lines trying to interpret the results (make sense of the difference). E.g. People with schizophrenia tend to have high-pitched voice, and present bigger swings in their prosody than controls. Add a couple of lines describing limitations of the data/analyses if any is relevant.

N.B. There are looots of files to be dealt with. Probably too many for your computer. This is a challenge for you. Some (complementary) possible strategies: You can select a subset of files only (and you have to justify your choice). You can learn how to use the apply() or map() functions. You can coordinate with classmates.

1. In the course of this assignment you have to first select one datafile and figure out how to:

```{r}
#load all libraries here
library(plyr)
library(crqa)
library(magrittr)
getwd()


#Load one datafile for excercise 1 and testing functions
data_1 = read.table("Pitch/Study2D0S209T4_f0.txt", header = T)

#save names of all files in Pitch folder (in format: Pitch/Study...txt so it can be loaded without changing wd)
pitch_files=list.files(path="Pitch",full.names = T)
```

```{r}
#get info from the name of the file
df_name = "Pitch/Study4D1S420T4_f0.txt"

#cut off string and keep only from 16th letter to 18th
Subject=as.numeric(substr(df_name, 16, 18))

Diagnosis=as.factor(substr(df_name,14,14))

Trial=substr(df_name,20,21)

Study=as.numeric(substr(df_name,12,12))

#bind together into dataframe
info=data.frame(Subject,Diagnosis,Trial,Study)
```

- Extract "standard" descriptors of pitch: Mean, standard deviation, range
```{r}
#get standard descriptors
mean=mean(data_1$f0)

sd=sd(data_1$f0)

#gets a vector of minimum and maximum
range=range(data_1$f0)
min=range[1] #first number in vector is minimun
max=range[2]

#bind together
standard=data.frame(mean,sd,min,max)
```

- Extract less "standard" descriptors of pitch you can think of (e.g. median, iqr, mean absoluted deviation, coefficient of variation)
```{r}
median=median(data_1$f0)

iqr=IQR(data_1$f0)

abs_mean_dev=mean(abs(data_1$f0-mean(data_1$f0)))

cof_var = sd(data_1$f0, na.rm=TRUE)/mean(data_1$f0, na.rm=TRUE)*100 #get coefficient of variation as percent

#bind together
less_standard=data.frame(median,iqr,abs_mean_dev,cof_var)
```


- Extract "complex" descriptors: recurrence quantification analysis
```{r}
#list needed for optimizeParam function
par = list(lgM =  50, steps = seq(1, 6, 1),  radiusspan = 100,  radiussample = 40, normalize = 0,  rescale = 0,  mindiagline = 2,  minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE,  fnnpercent = 10,  typeami = "mindip")

#get parameters for rqa - delay, emddim and radius
parameters = optimizeParam(data_1$f0,data_1$f0, par, min.rec = 3.5, max.rec = 4.5)
parameters

#perform rqa - file needs to be there twice because crqa is designed for comparing two timeseries)
results=crqa(data_1$f0,data_1$f0,delay=parameters$delay,embed=parameters$emddim,radius=parameters$radius,normalize=0,rescale=0,mindiagline = 2,minvertline = 2)
names(results)

#save rqa variables to separate variables
RR = results$RR
DET = results$DET
maxL = results$maxL #maximal trajectory
L = results$L #mean trajectory
ENTR = results$ENTR
LAM=results$LAM
TT = results$TT

#bind together
rqa = data.frame(RR, DET, maxL, L, ENTR, LAM, TT)
```

```{r}
#cbind extracted info and merge with demo data
data_desc=cbind(info, standard, less_standard, rqa)
```

2. Second you will have to turn the code into a function and loop through all the files (or even better use apply/sapply/lapply)
- Remember to extract the relevant information from the file names (Participant, Diagnosis, Trial, Study)

```{r}
#1. Wrap all the code from excercise 1 to functions
# extract info about subject from name of the file
get_info=function(file_name) {
  
  Subject=as.numeric(substr(file_name, 16, 18))
  Diagnosis=as.factor(substr(file_name,14,14))
  Trial=substr(file_name,20,21)
  Study=as.numeric(substr(file_name,12,12))
  
  info_df=data.frame(Subject,Diagnosis,Trial,Study)
  
  return(info_df)
}

#test the function 
get_info(df_name) #Works
```

```{r}
#Chunk 1: turn that code to a function
#Function 1: extract standard descriptors
get_stand_desc= function (df) {
  
  mean=mean(df$f0)
  sd=sd(df$f0)
  range=range(df$f0)
  min=range[1]
  max=range[2]
  
  stand_desc=data.frame(mean,sd,min,max)
  
  return(stand_desc)
  
}

#test function
get_stand_desc(data_1) #Works
```

```{r}
#Chunk 2: turned to function

get_less_stand_desc = function (df) {
  median=median(df$f0)
  iqr=IQR(df$f0)
  abs_mean_dev=mean(abs(df$f0-mean(df$f0)))
  cof_var = sd(df$f0, na.rm=TRUE)/mean(df$f0, na.rm=TRUE)*100
  less_stand_desc=data.frame(median,iqr,abs_mean_dev,cof_var)
  return(less_stand_desc)
}

#test function
get_less_stand_desc(data_1) #Works
```

```{r}
#rqa needs first the list of parameters - get optimal parameters here

#LOOP through all datafiles and get parameters from them - then get mean or median of all parameters to get the optimal ones that can be used for all datafiles

param_all=data.frame()

for (i in pitch_files) {
  data=read.table(file=i,header=T)
  
  #list needed for running the optimizeParam function
  par = list(lgM =  50, steps = seq(1, 6, 1),  radiusspan = 100,  radiussample = 40, normalize = 0,  rescale = 0,  mindiagline = 2, minvertline = 2,  tw = 0,  whiteline = FALSE,  recpt = FALSE,  fnnpercent = 10,  typeami = "mindip")
  
  param = 
    tryCatch(
    
      { #this is the try function - if it fails then error function will be run
        optimizeParam(data$f0,data$f0, par, min.rec = 3.5, max.rec = 4.5)
    },
   #move to this part if the try part fails
     error=function(cond){
        #return results with only NAs
        parameters_fail=list(radius=NA,emddim=NA,delay=NA)
        return(parameters_fail)
        
      }) #end of tryCatch
  
  #save output of optimizeParam to variables - either 
  radius=param$radius
  emddim=param$emddim
  delay=param$delay
  
  #bind these variables to dataframe
  param_df=data.frame(radius,emddim,delay)
  param_all=rbind(param_df,param_all)
}

#remove NAs from output of the loop
param_all = na.omit(param_all)

#summarize param_all to final df with mean
parameters_mean = param_all %>%
  summarize(radius=mean(radius),emddim=mean(emddim),delay=mean(delay))

#with median => seems better - numbers are similar but emddim is without decimals :-)
parameters_median = param_all %>%
  summarize(radius=median(radius),emddim=median(emddim),delay=median(delay))

#look at results
parameters_mean #taking delay from here - shorter delay can reduce number of NAs because some ts are very short, shorter than delay
parameters_median #taking emddim because it's without decimals - needs to be and also radius taking from here because its lower and therefore should maintain RR closer to 4%

parameters_final = list(radius=parameters_median$radius ,emddim=parameters_median$emddim ,delay=parameters_mean$delay)
```

```{r}
#Chunk 3 turned into function +tryCatch implented not to crash the loop
get_rqa= function (df,ans) {
  results=
    tryCatch(
      #this is the try part if it gets error here it will move to the error part
      {crqa(df$f0,df$f0,delay=ans$delay,embed=ans$emddim,radius=ans$radius,normalize=0,rescale=0,mindiagline = 2,minvertline = 2)
        },
      #error part - if function fails this function will be executed instead
      error=function(cond){
        #return results with only NAs
        results_fail=data.frame(RR=NA,DET=NA,maxL=NA,L=NA,ENTR=NA,LAM=NA,TT=NA)
        return(results_fail)
      }
  )
  RR = results$RR
  DET = results$DET
  maxL = results$maxL #maximal trajectory
  L = results$L #mean trajectory
  ENTR = results$ENTR
  LAM=results$LAM
  TT = results$TT
  rqa_df = data.frame(RR,DET,maxL,L,ENTR,LAM,TT)
  return(rqa_df)
}

#test the function
get_rqa(data_1,parameters_final) #Works and also RR is close to 4% with the common parameters
```

```{r final loop}
#let's loop through all files, extract the variables and rbind them all to one mega dataframe
data_all=data.frame()

for(i in pitch_files) {
  data=read.table(file=i,header=T)
  info=get_info(i)
  standard=get_stand_desc(data)
  less_standard=get_less_stand_desc(data)
  rqa=get_rqa(data,parameters_final)
  complete_df=cbind(info,standard,less_standard,rqa)
  data_all=rbind(data_all,complete_df)
}

#save the data to csv file so I don't need to run the loop again
write.csv(data_all, file = "data_analysis.csv",row.names = F)
```

```{r tidy data}
#load the final data to be independent of the first part of the code
final_data = read.csv("data_analysis.csv",header=T)

#load demographic data
demo = read.table("DemoData.txt", header= T)

#

#the final data need some tidying before merging with demo

#turn everything that should be factor to factor and numeric what should be numeric
final_data$Diagnosis= as.factor(final_data$Diagnosis)

#rename levels of Diagnosis from 1 and 0 to Scizophrenia and Control
final_data$Diagnosis = plyr::revalue(final_data$Diagnosis, c('0'="Control", "1"="Schizophrenia"))

#remove the underscore from the trial numbers
library(stringr)
final_data$Trial=str_replace(final_data$Trial,"_","")

#merge with demo data
complete_data = merge(final_data,demo, by=c("Subject", "Diagnosis", "Study"))

#combine ID and Diagnosis to new column to distinguish between schizophrenics and controls (each pair has the same ID)
library(tidyr)
complete_data= unite(complete_data, ID, c(Subject,Diagnosis),remove=F,sep="_")
complete_data$range = complete_data$max-complete_data$min

#save the complete file just to be sure it doesn't go missing
write.csv(complete_data, "complete_data.csv",row.names = F)
```

3. Make one model per acoustic feature and test whether you can observe significant difference due to Diagnosis. Tip: Which other fixed factors should you control for (that is, include in the model)? Which random ones?

```{r setup}
#start over here so that you dont need to run the whole code every time
library(ggplot2)
library(lmerTest)
library(magrittr)
library(modelr)
library(MuMIn)
library(caret)

#load complete data
data_acoustic=read.csv("complete_data.csv")

#turn variables to correct class
data_acoustic$Subject = as.factor(data_acoustic$Subject)

```

```{r make models}
model_mean = lmer(mean ~Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) #nope
summary(model_mean)

model_sd = lmer(sd ~ Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) #nope
summary(model_sd)

model_range = lme4::lmer(range ~Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) # yes - Dia and Study
summary(model_range)

model_median = lmer(median ~ Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) #nope
summary(model_median)

model_iqr = lmer(iqr ~ Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) #nope
summary(model_iqr)

model_absmeandev = lmer(abs_mean_dev ~ Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) #nope
summary(model_absmeandev)

model_cofvar = lmer(cof_var ~Diagnosis+Trial+Study+(1+Trial|ID),data_acoustic,REML=F) #yes Dia
summary(model_cofvar)

model_RR = lmer(RR ~ Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) #yes Dia
summary(model_RR)

model_DET= lmer(DET ~Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) # yes Dia
summary(model_DET)

model_maxL = lmer(maxL ~ Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) # yes Study
summary(model_maxL)

model_L = lmer(L ~ Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) #nope
summary(model_L)

model_ENTR = lmer(ENTR ~ Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) #yes Dia andStudy
summary(model_ENTR)

model_LAM = lmer(LAM ~ Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) # yes - Dia and Study 
summary(model_LAM)

model_TT = lmer(TT ~ Diagnosis+Trial+Study+(1+Trial|ID),na.omit(data_acoustic),REML=F) # yes Study
summary(model_TT)
```
- Bonus points:cross-validate the model and report the betas and standard errors from all rounds to get an idea of how robust the estimates are. 

I decided to cross-validate only models that showed significant effect of diagnosis since that's the main effect of interest. Significant were; model_range, model_cofvar, model_RR, model_DET, model_ENTR and model_LAM.
```{r}
#which models to crossvalidate? model_range, model_cofvar, model_RR, model_DET, model_ENTR and model_LAM

#come back to this if you have time
```

3a. Is study a significant predictor in these models? What should you infer from this? Does study interact with diagnosis? What should you infer from this?

Yes, study is a significant predictor some of these models. This difference could be explained by differences in the study design. For example the setting could be different or different stimuli might have been used which would affect all participants. 
```{r interaction of study}
#is there interaction of study and diagnosis?
#add interaction only where study or diagnosis were significant
model_range_inter = lmer(range ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_range_inter) #no

model_cofvar_inter = lmer(cof_var ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_cofvar_inter) #no

model_RR_inter = lmer(RR ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_RR_inter) #no

model_DET_inter = lmer(DET ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_DET_inter) #yes

model_maxL_inter = lmer(maxL ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_maxL_inter) #no

model_ENTR_inter = lmer(ENTR ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_ENTR_inter) #yes

model_LAM_inter = lmer(LAM ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_LAM_inter) #yes

model_TT_inter = lmer(TT ~Diagnosis+Trial+Study*Diagnosis+(1+Trial|ID),data_acoustic,REML=F)
summary(model_TT_inter) #no
```

In 3 models there was a significant interaction Study*Diagnosis; maxL Entropy and LAM models. This interaction shows that the different studies were differently designed and these differences influenced the participants differently based on their Diagnosis.

4. Bonus Question: Compare effect size of diagnosis across the different measures. Which measure seems most sensitive?
- Tip: to compare across measures you need to put all of them on the same scale, that is, you need to "standardize" them (z-score)

After looking in the lme4 package documentation I found out that t-values reported in summary of lmer model are identical to z-scores reported by glmer and therefore I can simply compare the t-values of the models as they are.

```{r compare effect sizes}
#compare t-values of my models significant
coef(summary(model_range))["DiagnosisSchizophrenia","t value"] 

model_list=list(model_range, model_cofvar, model_RR, model_DET, model_ENTR,model_LAM)

lapply(model_list,function(x) coef(summary(x))["DiagnosisSchizophrenia","t value"]) #coefficient of variance is the most sensitive with -3.624446 second is LAM
```